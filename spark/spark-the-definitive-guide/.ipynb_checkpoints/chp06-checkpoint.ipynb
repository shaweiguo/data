{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('csv').option('header', 'true').option('inferSchema', 'true')\\\n",
    "    .load('file:///home/sha/dev/books/Spark-The-Definitive-Guide/data/retail-data/by-day/2010-12-01.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('dfTable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      null|United Kingdom|\n",
      "|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      null|United Kingdom|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, instr, expr\n",
    "priceFilter = col(\"UnitPrice\") > 600\n",
    "descripFilter = instr(df.Description, \"POSTAGE\") >= 1\n",
    "df.where(df.StockCode.isin(\"DOT\")).where(priceFilter | descripFilter).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|unitPrice|isExpensive|\n",
      "+---------+-----------+\n",
      "|   569.77|       true|\n",
      "|   607.49|       true|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DOTCodeFilter = col('StockCode') == 'DOT'\n",
    "priceFilter = col('UnitPrice') > 600\n",
    "descripFilter = instr(col('Description'), 'POSTAGE') >= 1\n",
    "df.withColumn('isExpensive', DOTCodeFilter & (priceFilter | descripFilter)).where('isExpensive').select('unitPrice', 'isExpensive').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+\n",
      "|   Description|UnitPrice|\n",
      "+--------------+---------+\n",
      "|DOTCOM POSTAGE|   569.77|\n",
      "|DOTCOM POSTAGE|   607.49|\n",
      "+--------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('isExpensive', expr('NOT UnitPrice <= 250')).where('isExpensive').select('Description', 'UnitPrice').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|CustomerId|      realQuantity|\n",
      "+----------+------------------+\n",
      "|   17850.0|239.08999999999997|\n",
      "|   17850.0|          418.7156|\n",
      "+----------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import pow\n",
    "fabricatedQuantity = pow(col('Quantity') * col('UnitPrice'), 2) + 5\n",
    "df.select(expr('CustomerId'), fabricatedQuantity.alias('realQuantity')).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|CustomerId|      realQuantity|\n",
      "+----------+------------------+\n",
      "|   17850.0|239.08999999999997|\n",
      "|   17850.0|          418.7156|\n",
      "+----------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr('CustomerId', '(POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+\n",
      "|round(2.5, 0)|bround(2.5, 0)|\n",
      "+-------------+--------------+\n",
      "|          3.0|           2.0|\n",
      "|          3.0|           2.0|\n",
      "+-------------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, round, bround\n",
    "df.select(round(lit('2.5')), bround(lit('2.5'))).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|corr(Quantity, UnitPrice)|\n",
      "+-------------------------+\n",
      "|     -0.04112314436835551|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import corr\n",
    "df.stat.corr('Quantity', 'UnitPrice')\n",
    "df.select(corr('Quantity', 'UnitPrice')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+\n",
      "|summary|        InvoiceNo|         StockCode|         Description|          Quantity|         UnitPrice|        CustomerID|       Country|\n",
      "+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+\n",
      "|  count|             3108|              3108|                3098|              3108|              3108|              1968|          3108|\n",
      "|   mean| 536516.684944841|27834.304044117645|                null| 8.627413127413128| 4.151946589446603|15661.388719512195|          null|\n",
      "| stddev|72.89447869788873|17407.897548583845|                null|26.371821677029203|15.638659854603892|1854.4496996893627|          null|\n",
      "|    min|           536365|             10002| 4 PURPLE FLOCK D...|               -24|               0.0|           12431.0|     Australia|\n",
      "|    max|          C536548|              POST|ZINC WILLIE WINKI...|               600|            607.49|           18229.0|United Kingdom|\n",
      "+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|StockCode_Quantity| -1|-10|-12| -2|-24| -3| -4| -5| -6| -7|  1| 10|100| 11| 12|120|128| 13| 14|144| 15| 16| 17| 18| 19|192|  2| 20|200| 21|216| 22| 23| 24| 25|252| 27| 28|288|  3| 30| 32| 33| 34| 36|384|  4| 40|432| 47| 48|480|  5| 50| 56|  6| 60|600| 64|  7| 70| 72|  8| 80|  9| 96|\n",
      "+------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|             22578|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             21327|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22064|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             21080|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|\n",
      "|             22219|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             21908|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22818|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|           15056BL|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             72817|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22545|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22988|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|\n",
      "|             22274|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             20750|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|            82616C|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             21703|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22899|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|\n",
      "|             22379|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22422|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22769|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22585|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "+------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.stat.crosstab('StockCode', 'Quantity').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 1.65, 2.51, 4.21, 607.49]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colName = 'UnitPrice'\n",
    "quantileProbs = [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "relError = 0.05\n",
    "df.stat.approxQuantile('UnitPrice', quantileProbs, relError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|monotonically_increasing_id()|\n",
      "+-----------------------------+\n",
      "|                            0|\n",
      "|                            1|\n",
      "+-----------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "df.select(monotonically_increasing_id()).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|initcap(Description)|\n",
      "+--------------------+\n",
      "|White Hanging Hea...|\n",
      "| White Metal Lantern|\n",
      "|Cream Cupid Heart...|\n",
      "|Knitted Union Fla...|\n",
      "|Red Woolly Hottie...|\n",
      "|Set 7 Babushka Ne...|\n",
      "|Glass Star Froste...|\n",
      "|Hand Warmer Union...|\n",
      "|Hand Warmer Red P...|\n",
      "|Assorted Colour B...|\n",
      "|Poppy's Playhouse...|\n",
      "|Poppy's Playhouse...|\n",
      "|Feltcraft Princes...|\n",
      "|Ivory Knitted Mug...|\n",
      "|Box Of 6 Assorted...|\n",
      "|Box Of Vintage Ji...|\n",
      "|Box Of Vintage Al...|\n",
      "|Home Building Blo...|\n",
      "|Love Building Blo...|\n",
      "|Recipe Box With M...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import initcap\n",
    "df.select(initcap(col('Description'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------------+\n",
      "|         Description|  lower(Description)|upper(lower(Description))|\n",
      "+--------------------+--------------------+-------------------------+\n",
      "|WHITE HANGING HEA...|white hanging hea...|     WHITE HANGING HEA...|\n",
      "| WHITE METAL LANTERN| white metal lantern|      WHITE METAL LANTERN|\n",
      "+--------------------+--------------------+-------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lower, upper\n",
    "df.select(col('Description'), lower(col('Description')), upper(lower(col('Description')))).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----+----+----------+\n",
      "|    ltrim|    rtrim| trim|lpad|      rpad|\n",
      "+---------+---------+-----+----+----------+\n",
      "|Hello    |    Hello|Hello| Hel|Hello*****|\n",
      "|Hello    |    Hello|Hello| Hel|Hello*****|\n",
      "+---------+---------+-----+----+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import ltrim, rtrim, rpad, lpad, trim\n",
    "df.select(ltrim(lit('    Hello    ')).alias('ltrim'),\n",
    "         rtrim(lit('    Hello    ')).alias('rtrim'),\n",
    "         trim(lit('    Hello    ')).alias('trim'),\n",
    "         lpad(lit('Hello'), 3, '*').alias('lpad'),\n",
    "         rpad(lit('Hello'), 10, '*').alias('rpad')).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|         color_clean|         Description|\n",
      "+--------------------+--------------------+\n",
      "|COLOR HANGING HEA...|WHITE HANGING HEA...|\n",
      "| COLOR METAL LANTERN| WHITE METAL LANTERN|\n",
      "+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "regex_string = 'BLACK|WHITE|RED|GREEN|BLUE'\n",
    "df.select(\n",
    "    regexp_replace(col('Description'), regex_string, 'COLOR').alias('color_clean'),\n",
    "    col('Description')).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+--------------------+\n",
      "|translate(Description, LEET, 1337)|         Description|\n",
      "+----------------------------------+--------------------+\n",
      "|              WHI73 HANGING H3A...|WHITE HANGING HEA...|\n",
      "|               WHI73 M37A1 1AN73RN| WHITE METAL LANTERN|\n",
      "+----------------------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import translate\n",
    "df.select(translate(col('Description'), 'LEET', '1337'), col('Description')).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|color_clean|         Description|\n",
      "+-----------+--------------------+\n",
      "|      WHITE|WHITE HANGING HEA...|\n",
      "|      WHITE| WHITE METAL LANTERN|\n",
      "+-----------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_extract\n",
    "extract_str = '(BLACK|WHITE|RED|GREEN|BLUE)'\n",
    "df.select(\n",
    "    regexp_extract(col('Description'), extract_str, 1).alias('color_clean'), col('Description')).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|Description                       |\n",
      "+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHITE METAL LANTERN               |\n",
      "|RED WOOLLY HOTTIE WHITE HEART.    |\n",
      "+----------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import instr\n",
    "containsBlack = instr(col('Description'), 'BLACK') >= 1\n",
    "containsWhite = instr(col('Description'), 'WHITE') >= 1\n",
    "df.withColumn('hasSimpleColor', containsBlack | containsWhite).where('hasSimpleColor').select('Description').show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|Description                       |\n",
      "+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHITE METAL LANTERN               |\n",
      "|RED WOOLLY HOTTIE WHITE HEART.    |\n",
      "+----------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, locate\n",
    "simpleColors = ['black', 'white', 'red', 'green', 'blue']\n",
    "def color_locator(column, color_string):\n",
    "    return locate(color_string.upper(), column).cast('boolean').alias('is_' + color_string)\n",
    "selectedColumns = [color_locator(df.Description, c) for c in simpleColors]\n",
    "selectedColumns.append(expr('*'))\n",
    "\n",
    "df.select(*selectedColumns).where(expr('is_white OR is_red')).select('Description').show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- today: date (nullable = false)\n",
      " |-- now: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_date, current_timestamp\n",
    "dateDF = spark.range(10).withColumn('today', current_date()).withColumn('now', current_timestamp())\n",
    "dateDF.createOrReplaceTempView('dateTable')\n",
    "dateDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|date_sub(today, 5)|date_add(today, 5)|\n",
      "+------------------+------------------+\n",
      "|        2018-12-22|        2019-01-01|\n",
      "+------------------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_add, date_sub\n",
    "dateDF.select(date_sub(col('today'), 5), date_add(col('today'), 5)).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|datediff(week_ago, today)|\n",
      "+-------------------------+\n",
      "|                       -7|\n",
      "+-------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import datediff, months_between, to_date\n",
    "dateDF.withColumn('week_ago', date_sub(col('today'), 7)).select(datediff(col('week_ago'), col('today'))).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|months_between(start, end, true)|\n",
      "+--------------------------------+\n",
      "|                    -16.67741935|\n",
      "+--------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateDF.select(to_date(lit('2016-01-01')).alias('start'), to_date(lit('2017-05-22')).alias('end')).select(months_between(col('start'), col('end'))).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n",
      "|coalesce(Description, CustomerId)|\n",
      "+---------------------------------+\n",
      "|             WHITE HANGING HEA...|\n",
      "|              WHITE METAL LANTERN|\n",
      "|             CREAM CUPID HEART...|\n",
      "|             KNITTED UNION FLA...|\n",
      "|             RED WOOLLY HOTTIE...|\n",
      "|             SET 7 BABUSHKA NE...|\n",
      "|             GLASS STAR FROSTE...|\n",
      "|             HAND WARMER UNION...|\n",
      "|             HAND WARMER RED P...|\n",
      "|             ASSORTED COLOUR B...|\n",
      "|             POPPY'S PLAYHOUSE...|\n",
      "|             POPPY'S PLAYHOUSE...|\n",
      "|             FELTCRAFT PRINCES...|\n",
      "|             IVORY KNITTED MUG...|\n",
      "|             BOX OF 6 ASSORTED...|\n",
      "|             BOX OF VINTAGE JI...|\n",
      "|             BOX OF VINTAGE AL...|\n",
      "|             HOME BUILDING BLO...|\n",
      "|             LOVE BUILDING BLO...|\n",
      "|             RECIPE BOX WITH M...|\n",
      "+---------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import coalesce\n",
    "df.select(coalesce(col('Description'), col('CustomerId'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cDf = spark.createDataFrame([(None, None), (1, None), (None, 2)], (\"a\", \"b\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|   a|   b|\n",
      "+----+----+\n",
      "|null|null|\n",
      "|   1|null|\n",
      "|null|   2|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|coalesce(a, b)|\n",
      "+--------------+\n",
      "|          null|\n",
      "|             1|\n",
      "|             2|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cDf.select(coalesce(cDf[\"a\"], cDf[\"b\"])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dd = cDf.na.drop('any')\n",
    "dd.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|2010-12-01 08:26:00|     7.65|   17850.0|United Kingdom|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|2010-12-01 08:26:00|     4.25|   17850.0|United Kingdom|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|2010-12-01 08:28:00|     1.85|   17850.0|United Kingdom|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|2010-12-01 08:28:00|     1.85|   17850.0|United Kingdom|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|2010-12-01 08:34:00|     1.69|   13047.0|United Kingdom|\n",
      "|   536367|    22745|POPPY'S PLAYHOUSE...|       6|2010-12-01 08:34:00|      2.1|   13047.0|United Kingdom|\n",
      "|   536367|    22748|POPPY'S PLAYHOUSE...|       6|2010-12-01 08:34:00|      2.1|   13047.0|United Kingdom|\n",
      "|   536367|    22749|FELTCRAFT PRINCES...|       8|2010-12-01 08:34:00|     3.75|   13047.0|United Kingdom|\n",
      "|   536367|    22310|IVORY KNITTED MUG...|       6|2010-12-01 08:34:00|     1.65|   13047.0|United Kingdom|\n",
      "|   536367|    84969|BOX OF 6 ASSORTED...|       6|2010-12-01 08:34:00|     4.25|   13047.0|United Kingdom|\n",
      "|   536367|    22623|BOX OF VINTAGE JI...|       3|2010-12-01 08:34:00|     4.95|   13047.0|United Kingdom|\n",
      "|   536367|    22622|BOX OF VINTAGE AL...|       2|2010-12-01 08:34:00|     9.95|   13047.0|United Kingdom|\n",
      "|   536367|    21754|HOME BUILDING BLO...|       3|2010-12-01 08:34:00|     5.95|   13047.0|United Kingdom|\n",
      "|   536367|    21755|LOVE BUILDING BLO...|       3|2010-12-01 08:34:00|     5.95|   13047.0|United Kingdom|\n",
      "|   536367|    21777|RECIPE BOX WITH M...|       4|2010-12-01 08:34:00|     7.95|   13047.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dd = df.na.drop('all', subset=['StockCode', 'InvoiceNo'])\n",
    "dd.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|2010-12-01 08:26:00|     7.65|   17850.0|United Kingdom|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|2010-12-01 08:26:00|     4.25|   17850.0|United Kingdom|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|2010-12-01 08:28:00|     1.85|   17850.0|United Kingdom|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|2010-12-01 08:28:00|     1.85|   17850.0|United Kingdom|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|2010-12-01 08:34:00|     1.69|   13047.0|United Kingdom|\n",
      "|   536367|    22745|POPPY'S PLAYHOUSE...|       6|2010-12-01 08:34:00|      2.1|   13047.0|United Kingdom|\n",
      "|   536367|    22748|POPPY'S PLAYHOUSE...|       6|2010-12-01 08:34:00|      2.1|   13047.0|United Kingdom|\n",
      "|   536367|    22749|FELTCRAFT PRINCES...|       8|2010-12-01 08:34:00|     3.75|   13047.0|United Kingdom|\n",
      "|   536367|    22310|IVORY KNITTED MUG...|       6|2010-12-01 08:34:00|     1.65|   13047.0|United Kingdom|\n",
      "|   536367|    84969|BOX OF 6 ASSORTED...|       6|2010-12-01 08:34:00|     4.25|   13047.0|United Kingdom|\n",
      "|   536367|    22623|BOX OF VINTAGE JI...|       3|2010-12-01 08:34:00|     4.95|   13047.0|United Kingdom|\n",
      "|   536367|    22622|BOX OF VINTAGE AL...|       2|2010-12-01 08:34:00|     9.95|   13047.0|United Kingdom|\n",
      "|   536367|    21754|HOME BUILDING BLO...|       3|2010-12-01 08:34:00|     5.95|   13047.0|United Kingdom|\n",
      "|   536367|    21755|LOVE BUILDING BLO...|       3|2010-12-01 08:34:00|     5.95|   13047.0|United Kingdom|\n",
      "|   536367|    21777|RECIPE BOX WITH M...|       4|2010-12-01 08:34:00|     7.95|   13047.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dd = df.na.fill('all', subset=['StockCode', 'InvoiceNo'])\n",
    "dd.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|InvoiceNo|Description|\n",
      "+---------+-----------+\n",
      "|   536414|   No Value|\n",
      "|   536545|   No Value|\n",
      "|   536546|   No Value|\n",
      "|   536547|   No Value|\n",
      "|   536549|   No Value|\n",
      "|   536550|   No Value|\n",
      "|   536552|   No Value|\n",
      "|   536553|   No Value|\n",
      "|   536554|   No Value|\n",
      "|   536589|   No Value|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fill_cols_vals = {'StockCode': 5, 'Description': 'No Value'}\n",
    "dd = df.na.fill(fill_cols_vals)\n",
    "dd.where(expr('Description = \"No Value\"')).select('InvoiceNo', 'Description').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|InvoiceNo|Description|\n",
      "+---------+-----------+\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(expr('Description = null')).select('InvoiceNo', 'Description').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|(Description = NULL)|\n",
      "+--------------------+\n",
      "|                null|\n",
      "|                null|\n",
      "|                null|\n",
      "|                null|\n",
      "|                null|\n",
      "|                null|\n",
      "|                null|\n",
      "|                null|\n",
      "|                null|\n",
      "|                null|\n",
      "|                null|\n",
      "|                null|\n",
      "|                null|\n",
      "|                null|\n",
      "|                null|\n",
      "|                null|\n",
      "|                null|\n",
      "|                null|\n",
      "|                null|\n",
      "|                null|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(expr('Description = null')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|InvoiceNo|Description|\n",
      "+---------+-----------+\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(col('Description') != None).select('InvoiceNo', 'Description').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|InvoiceNo|Description|\n",
      "+---------+-----------+\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.replace([\"\"], [\"UNKNOWN\"], 'Description').where(expr('Description = \"UNKNOWN\"')).select('InvoiceNo', 'Description').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "udfExampleDF = spark.range(5).toDF('num')\n",
    "\n",
    "def power3(double_value):\n",
    "    return double_value ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "power3udf = udf(power3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|power3(num)|\n",
      "+-----------+\n",
      "|          0|\n",
      "|          1|\n",
      "+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "udfExampleDF.select(power3udf(col('num'))).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.power3(double_value)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "spark.udf.register('power3py', power3, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|power3py(num)|\n",
      "+-------------+\n",
      "|         null|\n",
      "|         null|\n",
      "+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "udfExampleDF.selectExpr('power3py(num)').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3108"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('csv').option('header', 'true').option('inferSchema', 'true')\\\n",
    ".load('file:///home/sha/dev/books/Spark-The-Definitive-Guide/data/retail-data/all/*.csv')\\\n",
    ".coalesce(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541909"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(InvoiceNo='536365', StockCode='85123A', Description='WHITE HANGING HEART T-LIGHT HOLDER', Quantity=6, InvoiceDate='12/1/2010 8:26', UnitPrice=2.55, CustomerID=17850, Country='United Kingdom')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.cache()\n",
    "df.createOrReplaceTempView('dfTable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|count(StockCode)|\n",
      "+----------------+\n",
      "|          541909|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "df.select(count('StockCode')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT StockCode)|\n",
      "+-------------------------+\n",
      "|                     4070|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "df.select(countDistinct('StockCode')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|approx_count_distinct(StockCode)|\n",
      "+--------------------------------+\n",
      "|                            3364|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import approx_count_distinct\n",
    "df.select(approx_count_distinct('StockCode', 0.1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----------------------+\n",
      "|first(StockCode, false)|last(StockCode, false)|\n",
      "+-----------------------+----------------------+\n",
      "|                 85123A|                 22138|\n",
      "+-----------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import first, last\n",
    "df.select(first('StockCode'), last('StockCode')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|min(Quantity)|max(Quantity)|\n",
      "+-------------+-------------+\n",
      "|       -80995|        80995|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "df.select(min('Quantity'), max('Quantity')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|sum(Quantity)|\n",
      "+-------------+\n",
      "|      5176450|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, sumDistinct\n",
    "df.select(sum('Quantity')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|sum(DISTINCT Quantity)|\n",
      "+----------------------+\n",
      "|                 29310|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(sumDistinct('Quantity')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+----------------+----------------+\n",
      "|(total_purchases / total_transactions)|   avg_purchases|  mean_purchases|\n",
      "+--------------------------------------+----------------+----------------+\n",
      "|                      9.55224954743324|9.55224954743324|9.55224954743324|\n",
      "+--------------------------------------+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, expr\n",
    "df.select(\n",
    "    count('Quantity').alias('total_transactions'),\n",
    "    sum('Quantity').alias('total_purchases'),\n",
    "    avg('Quantity').alias('avg_purchases'),\n",
    "    expr('mean(Quantity)').alias('mean_purchases'))\\\n",
    ".selectExpr('total_purchases/total_transactions', 'avg_purchases', 'mean_purchases').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------------------+---------------------+\n",
      "| var_pop(Quantity)|var_samp(Quantity)|stddev_pop(Quantity)|stddev_samp(Quantity)|\n",
      "+------------------+------------------+--------------------+---------------------+\n",
      "|47559.303646609354| 47559.39140929905|  218.08095663447864|   218.08115785023486|\n",
      "+------------------+------------------+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import var_pop, stddev_pop\n",
    "from pyspark.sql.functions import var_samp, stddev_samp\n",
    "df.select(var_pop('Quantity'), var_samp('Quantity'),\n",
    "         stddev_pop('Quantity'), stddev_samp('Quantity')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|  skewness(Quantity)|kurtosis(Quantity)|\n",
      "+--------------------+------------------+\n",
      "|-0.26407557610527843|119768.05495536518|\n",
      "+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import skewness, kurtosis\n",
    "df.select(skewness('Quantity'), kurtosis('Quantity')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-------------------------------+------------------------------+\n",
      "|corr(InvoiceNo, Quantity)|covar_samp(InvoiceNo, Quantity)|covar_pop(InvoiceNo, Quantity)|\n",
      "+-------------------------+-------------------------------+------------------------------+\n",
      "|     4.912186085637639E-4|             1052.7280543913773|            1052.7260778752732|\n",
      "+-------------------------+-------------------------------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import corr, covar_pop, covar_samp\n",
    "df.select(corr('InvoiceNo', 'Quantity'), covar_samp('InvoiceNo', 'Quantity'), covar_pop('InvoiceNo', 'Quantity')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----+\n",
      "|InvoiceNo|CustomerId|count|\n",
      "+---------+----------+-----+\n",
      "|   536846|     14573|   76|\n",
      "|   537026|     12395|   12|\n",
      "|   537883|     14437|    5|\n",
      "|   538068|     17978|   12|\n",
      "|   538279|     14952|    7|\n",
      "|   538800|     16458|   10|\n",
      "|   538942|     17346|   12|\n",
      "|  C539947|     13854|    1|\n",
      "|   540096|     13253|   16|\n",
      "|   540530|     14755|   27|\n",
      "|   541225|     14099|   19|\n",
      "|   541978|     13551|    4|\n",
      "|   542093|     17677|   16|\n",
      "|   543188|     12567|   63|\n",
      "|   543590|     17377|   19|\n",
      "|  C543757|     13115|    1|\n",
      "|  C544318|     12989|    1|\n",
      "|   544578|     12365|    1|\n",
      "|   545165|     16339|   20|\n",
      "|   545289|     14732|   30|\n",
      "+---------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('InvoiceNo', 'CustomerId').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+---------------+\n",
      "|InvoiceNo|quan|count(Quantity)|\n",
      "+---------+----+---------------+\n",
      "|   536596|   6|              6|\n",
      "|   536938|  14|             14|\n",
      "|   537252|   1|              1|\n",
      "|   537691|  20|             20|\n",
      "|   538041|   1|              1|\n",
      "|   538184|  26|             26|\n",
      "|   538517|  53|             53|\n",
      "|   538879|  19|             19|\n",
      "|   539275|   6|              6|\n",
      "|   539630|  12|             12|\n",
      "|   540499|  24|             24|\n",
      "|   540540|  22|             22|\n",
      "|  C540850|   1|              1|\n",
      "|   540976|  48|             48|\n",
      "|   541432|   4|              4|\n",
      "|   541518| 101|            101|\n",
      "|   541783|  35|             35|\n",
      "|   542026|   9|              9|\n",
      "|   542375|   6|              6|\n",
      "|  C542604|   8|              8|\n",
      "+---------+----+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('InvoiceNo').agg(count('Quantity').alias('quan'), expr('count(Quantity)')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+--------------------+\n",
      "|InvoiceNo|     avg(Quantity)|stddev_pop(Quantity)|\n",
      "+---------+------------------+--------------------+\n",
      "|   536596|               1.5|  1.1180339887498947|\n",
      "|   536938|33.142857142857146|  20.698023172885524|\n",
      "|   537252|              31.0|                 0.0|\n",
      "|   537691|              8.15|   5.597097462078001|\n",
      "|   538041|              30.0|                 0.0|\n",
      "|   538184|12.076923076923077|   8.142590198943392|\n",
      "|   538517|3.0377358490566038|  2.3946659604837897|\n",
      "|   538879|21.157894736842106|  11.811070444356483|\n",
      "|   539275|              26.0|  12.806248474865697|\n",
      "|   539630|20.333333333333332|  10.225241100118645|\n",
      "|   540499|              3.75|  2.6653642652865788|\n",
      "|   540540|2.1363636363636362|  1.0572457590557278|\n",
      "|  C540850|              -1.0|                 0.0|\n",
      "|   540976|10.520833333333334|   6.496760677872902|\n",
      "|   541432|             12.25|  10.825317547305483|\n",
      "|   541518| 23.10891089108911|  20.550782784878713|\n",
      "|   541783|11.314285714285715|   8.467657556242811|\n",
      "|   542026| 7.666666666666667|   4.853406592853679|\n",
      "|   542375|               8.0|  3.4641016151377544|\n",
      "|  C542604|              -8.0|  15.173990905493518|\n",
      "+---------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('InvoiceNo').agg(expr('avg(Quantity)'), expr('stddev_pop(Quantity)')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "dfWithDate = df.withColumn('date', to_date(col('InvoiceDate'), 'MM/d/yyyy H:mm'))\n",
    "dfWithDate.createOrReplaceTempView('dfWithDate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpec = Window.partitionBy('CustomerId', 'date').orderBy(desc('Quantity'))\\\n",
    ".rowsBetween(Window.unboundedPreceding, Window.currentRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max\n",
    "maxPurchaseQuantity = max(col('Quantity')).over(windowSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import dense_rank, rank\n",
    "purchaseDenseRank = dense_rank().over(windowSpec)\n",
    "purchaseRank = rank().over(windowSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[CustomerId: int, date: date, Quantity: int]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfWithDate.where('CustomerId IS NULL').orderBy('CustomerId').select(col('CustomerId'), col('date'), col('Quantity'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+-----------------+-------------------+\n",
      "|CustomerId|      date|Quantity|quantityDenseRank|maxPurchaseQuantity|\n",
      "+----------+----------+--------+-----------------+-------------------+\n",
      "|     12346|2011-01-18|   74215|                1|              74215|\n",
      "|     12346|2011-01-18|  -74215|                2|              74215|\n",
      "|     12347|2010-12-07|      36|                1|                 36|\n",
      "|     12347|2010-12-07|      30|                2|                 36|\n",
      "|     12347|2010-12-07|      24|                3|                 36|\n",
      "|     12347|2010-12-07|      12|                4|                 36|\n",
      "|     12347|2010-12-07|      12|                4|                 36|\n",
      "|     12347|2010-12-07|      12|                4|                 36|\n",
      "|     12347|2010-12-07|      12|                4|                 36|\n",
      "|     12347|2010-12-07|      12|                4|                 36|\n",
      "|     12347|2010-12-07|      12|                4|                 36|\n",
      "|     12347|2010-12-07|      12|                4|                 36|\n",
      "|     12347|2010-12-07|      12|                4|                 36|\n",
      "|     12347|2010-12-07|      12|                4|                 36|\n",
      "|     12347|2010-12-07|      12|                4|                 36|\n",
      "|     12347|2010-12-07|      12|                4|                 36|\n",
      "|     12347|2010-12-07|      12|                4|                 36|\n",
      "|     12347|2010-12-07|      12|                4|                 36|\n",
      "|     12347|2010-12-07|       6|                5|                 36|\n",
      "|     12347|2010-12-07|       6|                5|                 36|\n",
      "+----------+----------+--------+-----------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithDate.where('CustomerId IS NOT NULL').orderBy('CustomerId')\\\n",
    "    .select(\n",
    "        col('CustomerId'),\n",
    "        col('date'),\n",
    "        col('Quantity'),\n",
    "        purchaseDenseRank.alias('quantityDenseRank'),\n",
    "        maxPurchaseQuantity.alias('maxPurchaseQuantity')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541909"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfWithDate.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNoNull = dfWithDate.drop()\n",
    "dfNoNull.createOrReplaceTempView('dfNoNull')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541909"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfNoNull.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+\n",
      "|      Date|       Country|total_quantity|\n",
      "+----------+--------------+--------------+\n",
      "|      null|          null|       5176450|\n",
      "|2010-12-01|       Germany|           117|\n",
      "|2010-12-01|   Netherlands|            97|\n",
      "|2010-12-01|        France|           449|\n",
      "|2010-12-01|     Australia|           107|\n",
      "|2010-12-01|          null|         26814|\n",
      "|2010-12-01|          EIRE|           243|\n",
      "|2010-12-01|        Norway|          1852|\n",
      "|2010-12-01|United Kingdom|         23949|\n",
      "|2010-12-02|       Germany|           146|\n",
      "|2010-12-02|          null|         21023|\n",
      "|2010-12-02|United Kingdom|         20873|\n",
      "|2010-12-02|          EIRE|             4|\n",
      "|2010-12-03|          null|         14830|\n",
      "|2010-12-03|          EIRE|          2575|\n",
      "|2010-12-03|       Germany|           170|\n",
      "|2010-12-03|   Switzerland|           110|\n",
      "|2010-12-03|       Belgium|           528|\n",
      "|2010-12-03|      Portugal|            65|\n",
      "|2010-12-03|        France|           239|\n",
      "+----------+--------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rolledUpDF = dfNoNull.rollup('Date', 'Country').agg(sum('Quantity'))\\\n",
    "    .selectExpr('Date', 'Country', '`sum(Quantity)` as total_quantity')\\\n",
    "    .orderBy('Date')\n",
    "rolledUpDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-------------+\n",
      "|Date|             Country|sum(Quantity)|\n",
      "+----+--------------------+-------------+\n",
      "|null|               Japan|        25218|\n",
      "|null|           Australia|        83653|\n",
      "|null|            Portugal|        16180|\n",
      "|null|                 RSA|          352|\n",
      "|null|           Hong Kong|         4769|\n",
      "|null|         Unspecified|         3300|\n",
      "|null|             Germany|       117448|\n",
      "|null|                 USA|         1034|\n",
      "|null|             Finland|        10666|\n",
      "|null|                null|      5176450|\n",
      "|null|           Singapore|         5234|\n",
      "|null|United Arab Emirates|          982|\n",
      "|null|              Cyprus|         6317|\n",
      "|null|               Spain|        26824|\n",
      "|null|     Channel Islands|         9479|\n",
      "|null|  European Community|          497|\n",
      "|null|              Norway|        19247|\n",
      "|null|             Denmark|         8188|\n",
      "|null|             Lebanon|          386|\n",
      "|null|      Czech Republic|          592|\n",
      "+----+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "dfNoNull.cube('Date', 'Country').agg(sum(col('Quantity')))\\\n",
    "    .select('Date', 'Country', 'sum(Quantity)').orderBy('Date').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfDateNoNull = dfNoNull.selectExpr('Date IS NOT NULL')\n",
    "# dfDateNoNull.cube('Date', 'Country').agg(sum(col('Quantity')))\\\n",
    "#     .select('Date', 'Country', 'sum(Quantity)').orderBy('Date').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted = dfWithDate.groupBy(\"date\").pivot(\"Country\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`USA_sum(Quantity)`' given input columns: [Cyprus_sum(CAST(Quantity AS BIGINT)), Australia_sum(UnitPrice), Canada_sum(UnitPrice), Lithuania_sum(CAST(Quantity AS BIGINT)), EIRE_sum(UnitPrice), Bahrain_sum(CAST(Quantity AS BIGINT)), Norway_sum(CAST(CustomerID AS BIGINT)), USA_sum(CAST(CustomerID AS BIGINT)), RSA_sum(CAST(Quantity AS BIGINT)), Canada_sum(CAST(CustomerID AS BIGINT)), United Kingdom_sum(CAST(CustomerID AS BIGINT)), Finland_sum(CAST(CustomerID AS BIGINT)), Israel_sum(CAST(CustomerID AS BIGINT)), Belgium_sum(CAST(Quantity AS BIGINT)), Lithuania_sum(CAST(CustomerID AS BIGINT)), Singapore_sum(CAST(CustomerID AS BIGINT)), Channel Islands_sum(UnitPrice), Belgium_sum(CAST(CustomerID AS BIGINT)), Malta_sum(CAST(Quantity AS BIGINT)), Netherlands_sum(CAST(CustomerID AS BIGINT)), Finland_sum(CAST(Quantity AS BIGINT)), Australia_sum(CAST(Quantity AS BIGINT)), Japan_sum(UnitPrice), Saudi Arabia_sum(UnitPrice), Switzerland_sum(CAST(CustomerID AS BIGINT)), United Arab Emirates_sum(CAST(CustomerID AS BIGINT)), Denmark_sum(CAST(CustomerID AS BIGINT)), Poland_sum(CAST(Quantity AS BIGINT)), Greece_sum(UnitPrice), Germany_sum(CAST(CustomerID AS BIGINT)), Netherlands_sum(UnitPrice), Portugal_sum(UnitPrice), Netherlands_sum(CAST(Quantity AS BIGINT)), Israel_sum(UnitPrice), Bahrain_sum(CAST(CustomerID AS BIGINT)), Iceland_sum(CAST(Quantity AS BIGINT)), Sweden_sum(UnitPrice), Israel_sum(CAST(Quantity AS BIGINT)), Switzerland_sum(UnitPrice), Norway_sum(UnitPrice), Hong Kong_sum(CAST(Quantity AS BIGINT)), Austria_sum(CAST(Quantity AS BIGINT)), Malta_sum(UnitPrice), United Kingdom_sum(UnitPrice), Australia_sum(CAST(CustomerID AS BIGINT)), Brazil_sum(UnitPrice), Belgium_sum(UnitPrice), France_sum(UnitPrice), Brazil_sum(CAST(CustomerID AS BIGINT)), Unspecified_sum(CAST(CustomerID AS BIGINT)), Unspecified_sum(UnitPrice), Hong Kong_sum(CAST(CustomerID AS BIGINT)), European Community_sum(UnitPrice), Spain_sum(CAST(Quantity AS BIGINT)), Japan_sum(CAST(CustomerID AS BIGINT)), Austria_sum(CAST(CustomerID AS BIGINT)), Czech Republic_sum(UnitPrice), Lithuania_sum(UnitPrice), Switzerland_sum(CAST(Quantity AS BIGINT)), Norway_sum(CAST(Quantity AS BIGINT)), Czech Republic_sum(CAST(CustomerID AS BIGINT)), Poland_sum(UnitPrice), Singapore_sum(UnitPrice), Italy_sum(CAST(CustomerID AS BIGINT)), Malta_sum(CAST(CustomerID AS BIGINT)), Spain_sum(UnitPrice), Sweden_sum(CAST(CustomerID AS BIGINT)), Poland_sum(CAST(CustomerID AS BIGINT)), Saudi Arabia_sum(CAST(Quantity AS BIGINT)), Denmark_sum(CAST(Quantity AS BIGINT)), USA_sum(CAST(Quantity AS BIGINT)), Germany_sum(CAST(Quantity AS BIGINT)), Spain_sum(CAST(CustomerID AS BIGINT)), date, Greece_sum(CAST(Quantity AS BIGINT)), United Arab Emirates_sum(UnitPrice), Lebanon_sum(UnitPrice), RSA_sum(CAST(CustomerID AS BIGINT)), Cyprus_sum(CAST(CustomerID AS BIGINT)), Saudi Arabia_sum(CAST(CustomerID AS BIGINT)), France_sum(CAST(Quantity AS BIGINT)), Hong Kong_sum(UnitPrice), RSA_sum(UnitPrice), Unspecified_sum(CAST(Quantity AS BIGINT)), Germany_sum(UnitPrice), Portugal_sum(CAST(CustomerID AS BIGINT)), European Community_sum(CAST(CustomerID AS BIGINT)), Iceland_sum(UnitPrice), Finland_sum(UnitPrice), France_sum(CAST(CustomerID AS BIGINT)), Brazil_sum(CAST(Quantity AS BIGINT)), Italy_sum(UnitPrice), Channel Islands_sum(CAST(Quantity AS BIGINT)), Denmark_sum(UnitPrice), Sweden_sum(CAST(Quantity AS BIGINT)), Portugal_sum(CAST(Quantity AS BIGINT)), Bahrain_sum(UnitPrice), EIRE_sum(CAST(Quantity AS BIGINT)), Austria_sum(UnitPrice), United Arab Emirates_sum(CAST(Quantity AS BIGINT)), Lebanon_sum(CAST(Quantity AS BIGINT)), Japan_sum(CAST(Quantity AS BIGINT)), United Kingdom_sum(CAST(Quantity AS BIGINT)), Canada_sum(CAST(Quantity AS BIGINT)), Iceland_sum(CAST(CustomerID AS BIGINT)), Czech Republic_sum(CAST(Quantity AS BIGINT)), Lebanon_sum(CAST(CustomerID AS BIGINT)), Italy_sum(CAST(Quantity AS BIGINT)), Channel Islands_sum(CAST(CustomerID AS BIGINT)), USA_sum(UnitPrice), Singapore_sum(CAST(Quantity AS BIGINT)), EIRE_sum(CAST(CustomerID AS BIGINT)), European Community_sum(CAST(Quantity AS BIGINT)), Cyprus_sum(UnitPrice), Greece_sum(CAST(CustomerID AS BIGINT))];;\\n'Project [date#3281, 'USA_sum(Quantity)]\\n+- Filter (cast(date#3281 as string) > 2011-12-05)\\n   +- Project [date#3281, __pivot_sum(CAST(`Quantity` AS BIGINT)) AS `sum(CAST(``Quantity`` AS BIGINT))`#3832[0] AS Australia_sum(CAST(Quantity AS BIGINT))#3989L, __pivot_sum(`UnitPrice`) AS `sum(``UnitPrice``)`#3910[0] AS Australia_sum(UnitPrice)#3990, __pivot_sum(CAST(`CustomerID` AS BIGINT)) AS `sum(CAST(``CustomerID`` AS BIGINT))`#3988[0] AS Australia_sum(CAST(CustomerID AS BIGINT))#3991L, __pivot_sum(CAST(`Quantity` AS BIGINT)) AS `sum(CAST(``Quantity`` AS BIGINT))`#3832[1] AS Austria_sum(CAST(Quantity AS BIGINT))#3992L, __pivot_sum(`UnitPrice`) AS `sum(``UnitPrice``)`#3910[1] AS Austria_sum(UnitPrice)#3993, __pivot_sum(CAST(`CustomerID` AS BIGINT)) AS `sum(CAST(``CustomerID`` AS BIGINT))`#3988[1] AS Austria_sum(CAST(CustomerID AS BIGINT))#3994L, __pivot_sum(CAST(`Quantity` AS BIGINT)) AS `sum(CAST(``Quantity`` AS BIGINT))`#3832[2] AS Bahrain_sum(CAST(Quantity AS BIGINT))#3995L, __pivot_sum(`UnitPrice`) AS `sum(``UnitPrice``)`#3910[2] AS Bahrain_sum(UnitPrice)#3996, __pivot_sum(CAST(`CustomerID` AS BIGINT)) AS `sum(CAST(``CustomerID`` AS BIGINT))`#3988[2] AS Bahrain_sum(CAST(CustomerID AS BIGINT))#3997L, __pivot_sum(CAST(`Quantity` AS BIGINT)) AS `sum(CAST(``Quantity`` AS BIGINT))`#3832[3] AS Belgium_sum(CAST(Quantity AS BIGINT))#3998L, __pivot_sum(`UnitPrice`) AS `sum(``UnitPrice``)`#3910[3] AS Belgium_sum(UnitPrice)#3999, __pivot_sum(CAST(`CustomerID` AS BIGINT)) AS `sum(CAST(``CustomerID`` AS BIGINT))`#3988[3] AS Belgium_sum(CAST(CustomerID AS BIGINT))#4000L, __pivot_sum(CAST(`Quantity` AS BIGINT)) AS `sum(CAST(``Quantity`` AS BIGINT))`#3832[4] AS Brazil_sum(CAST(Quantity AS BIGINT))#4001L, __pivot_sum(`UnitPrice`) AS `sum(``UnitPrice``)`#3910[4] AS Brazil_sum(UnitPrice)#4002, __pivot_sum(CAST(`CustomerID` AS BIGINT)) AS `sum(CAST(``CustomerID`` AS BIGINT))`#3988[4] AS Brazil_sum(CAST(CustomerID AS BIGINT))#4003L, __pivot_sum(CAST(`Quantity` AS BIGINT)) AS `sum(CAST(``Quantity`` AS BIGINT))`#3832[5] AS Canada_sum(CAST(Quantity AS BIGINT))#4004L, __pivot_sum(`UnitPrice`) AS `sum(``UnitPrice``)`#3910[5] AS Canada_sum(UnitPrice)#4005, __pivot_sum(CAST(`CustomerID` AS BIGINT)) AS `sum(CAST(``CustomerID`` AS BIGINT))`#3988[5] AS Canada_sum(CAST(CustomerID AS BIGINT))#4006L, __pivot_sum(CAST(`Quantity` AS BIGINT)) AS `sum(CAST(``Quantity`` AS BIGINT))`#3832[6] AS Channel Islands_sum(CAST(Quantity AS BIGINT))#4007L, __pivot_sum(`UnitPrice`) AS `sum(``UnitPrice``)`#3910[6] AS Channel Islands_sum(UnitPrice)#4008, __pivot_sum(CAST(`CustomerID` AS BIGINT)) AS `sum(CAST(``CustomerID`` AS BIGINT))`#3988[6] AS Channel Islands_sum(CAST(CustomerID AS BIGINT))#4009L, __pivot_sum(CAST(`Quantity` AS BIGINT)) AS `sum(CAST(``Quantity`` AS BIGINT))`#3832[7] AS Cyprus_sum(CAST(Quantity AS BIGINT))#4010L, __pivot_sum(`UnitPrice`) AS `sum(``UnitPrice``)`#3910[7] AS Cyprus_sum(UnitPrice)#4011, ... 91 more fields]\\n      +- Aggregate [date#3281], [date#3281, pivotfirst(Country#1659, sum(CAST(`Quantity` AS BIGINT))#3752L, Australia, Austria, Bahrain, Belgium, Brazil, Canada, Channel Islands, Cyprus, Czech Republic, Denmark, EIRE, European Community, Finland, France, Germany, Greece, Hong Kong, Iceland, Israel, Italy, Japan, Lebanon, Lithuania, Malta, Netherlands, Norway, Poland, Portugal, RSA, Saudi Arabia, Singapore, Spain, Sweden, Switzerland, USA, United Arab Emirates, United Kingdom, Unspecified, 0, 0) AS __pivot_sum(CAST(`Quantity` AS BIGINT)) AS `sum(CAST(``Quantity`` AS BIGINT))`#3832, pivotfirst(Country#1659, sum(`UnitPrice`)#3753, Australia, Austria, Bahrain, Belgium, Brazil, Canada, Channel Islands, Cyprus, Czech Republic, Denmark, EIRE, European Community, Finland, France, Germany, Greece, Hong Kong, Iceland, Israel, Italy, Japan, Lebanon, Lithuania, Malta, Netherlands, Norway, Poland, Portugal, RSA, Saudi Arabia, Singapore, Spain, Sweden, Switzerland, USA, United Arab Emirates, United Kingdom, Unspecified, 0, 0) AS __pivot_sum(`UnitPrice`) AS `sum(``UnitPrice``)`#3910, pivotfirst(Country#1659, sum(CAST(`CustomerID` AS BIGINT))#3754L, Australia, Austria, Bahrain, Belgium, Brazil, Canada, Channel Islands, Cyprus, Czech Republic, Denmark, EIRE, European Community, Finland, France, Germany, Greece, Hong Kong, Iceland, Israel, Italy, Japan, Lebanon, Lithuania, Malta, Netherlands, Norway, Poland, Portugal, RSA, Saudi Arabia, Singapore, Spain, Sweden, Switzerland, USA, United Arab Emirates, United Kingdom, Unspecified, 0, 0) AS __pivot_sum(CAST(`CustomerID` AS BIGINT)) AS `sum(CAST(``CustomerID`` AS BIGINT))`#3988]\\n         +- Aggregate [date#3281, Country#1659], [date#3281, Country#1659, sum(cast(Quantity#1655 as bigint)) AS sum(CAST(`Quantity` AS BIGINT))#3752L, sum(UnitPrice#1657) AS sum(`UnitPrice`)#3753, sum(cast(CustomerID#1658 as bigint)) AS sum(CAST(`CustomerID` AS BIGINT))#3754L]\\n            +- Project [InvoiceNo#1652, StockCode#1653, Description#1654, Quantity#1655, InvoiceDate#1656, UnitPrice#1657, CustomerID#1658, Country#1659, to_date('InvoiceDate, Some(MM/d/yyyy H:mm)) AS date#3281]\\n               +- Repartition 5, false\\n                  +- Relation[InvoiceNo#1652,StockCode#1653,Description#1654,Quantity#1655,InvoiceDate#1656,UnitPrice#1657,CustomerID#1658,Country#1659] csv\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/share/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/share/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o959.select.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`USA_sum(Quantity)`' given input columns: [Cyprus_sum(CAST(Quantity AS BIGINT)), Australia_sum(UnitPrice), Canada_sum(UnitPrice), Lithuania_sum(CAST(Quantity AS BIGINT)), EIRE_sum(UnitPrice), Bahrain_sum(CAST(Quantity AS BIGINT)), Norway_sum(CAST(CustomerID AS BIGINT)), USA_sum(CAST(CustomerID AS BIGINT)), RSA_sum(CAST(Quantity AS BIGINT)), Canada_sum(CAST(CustomerID AS BIGINT)), United Kingdom_sum(CAST(CustomerID AS BIGINT)), Finland_sum(CAST(CustomerID AS BIGINT)), Israel_sum(CAST(CustomerID AS BIGINT)), Belgium_sum(CAST(Quantity AS BIGINT)), Lithuania_sum(CAST(CustomerID AS BIGINT)), Singapore_sum(CAST(CustomerID AS BIGINT)), Channel Islands_sum(UnitPrice), Belgium_sum(CAST(CustomerID AS BIGINT)), Malta_sum(CAST(Quantity AS BIGINT)), Netherlands_sum(CAST(CustomerID AS BIGINT)), Finland_sum(CAST(Quantity AS BIGINT)), Australia_sum(CAST(Quantity AS BIGINT)), Japan_sum(UnitPrice), Saudi Arabia_sum(UnitPrice), Switzerland_sum(CAST(CustomerID AS BIGINT)), United Arab Emirates_sum(CAST(CustomerID AS BIGINT)), Denmark_sum(CAST(CustomerID AS BIGINT)), Poland_sum(CAST(Quantity AS BIGINT)), Greece_sum(UnitPrice), Germany_sum(CAST(CustomerID AS BIGINT)), Netherlands_sum(UnitPrice), Portugal_sum(UnitPrice), Netherlands_sum(CAST(Quantity AS BIGINT)), Israel_sum(UnitPrice), Bahrain_sum(CAST(CustomerID AS BIGINT)), Iceland_sum(CAST(Quantity AS BIGINT)), Sweden_sum(UnitPrice), Israel_sum(CAST(Quantity AS BIGINT)), Switzerland_sum(UnitPrice), Norway_sum(UnitPrice), Hong Kong_sum(CAST(Quantity AS BIGINT)), Austria_sum(CAST(Quantity AS BIGINT)), Malta_sum(UnitPrice), United Kingdom_sum(UnitPrice), Australia_sum(CAST(CustomerID AS BIGINT)), Brazil_sum(UnitPrice), Belgium_sum(UnitPrice), France_sum(UnitPrice), Brazil_sum(CAST(CustomerID AS BIGINT)), Unspecified_sum(CAST(CustomerID AS BIGINT)), Unspecified_sum(UnitPrice), Hong Kong_sum(CAST(CustomerID AS BIGINT)), European Community_sum(UnitPrice), Spain_sum(CAST(Quantity AS BIGINT)), Japan_sum(CAST(CustomerID AS BIGINT)), Austria_sum(CAST(CustomerID AS BIGINT)), Czech Republic_sum(UnitPrice), Lithuania_sum(UnitPrice), Switzerland_sum(CAST(Quantity AS BIGINT)), Norway_sum(CAST(Quantity AS BIGINT)), Czech Republic_sum(CAST(CustomerID AS BIGINT)), Poland_sum(UnitPrice), Singapore_sum(UnitPrice), Italy_sum(CAST(CustomerID AS BIGINT)), Malta_sum(CAST(CustomerID AS BIGINT)), Spain_sum(UnitPrice), Sweden_sum(CAST(CustomerID AS BIGINT)), Poland_sum(CAST(CustomerID AS BIGINT)), Saudi Arabia_sum(CAST(Quantity AS BIGINT)), Denmark_sum(CAST(Quantity AS BIGINT)), USA_sum(CAST(Quantity AS BIGINT)), Germany_sum(CAST(Quantity AS BIGINT)), Spain_sum(CAST(CustomerID AS BIGINT)), date, Greece_sum(CAST(Quantity AS BIGINT)), United Arab Emirates_sum(UnitPrice), Lebanon_sum(UnitPrice), RSA_sum(CAST(CustomerID AS BIGINT)), Cyprus_sum(CAST(CustomerID AS BIGINT)), Saudi Arabia_sum(CAST(CustomerID AS BIGINT)), France_sum(CAST(Quantity AS BIGINT)), Hong Kong_sum(UnitPrice), RSA_sum(UnitPrice), Unspecified_sum(CAST(Quantity AS BIGINT)), Germany_sum(UnitPrice), Portugal_sum(CAST(CustomerID AS BIGINT)), European Community_sum(CAST(CustomerID AS BIGINT)), Iceland_sum(UnitPrice), Finland_sum(UnitPrice), France_sum(CAST(CustomerID AS BIGINT)), Brazil_sum(CAST(Quantity AS BIGINT)), Italy_sum(UnitPrice), Channel Islands_sum(CAST(Quantity AS BIGINT)), Denmark_sum(UnitPrice), Sweden_sum(CAST(Quantity AS BIGINT)), Portugal_sum(CAST(Quantity AS BIGINT)), Bahrain_sum(UnitPrice), EIRE_sum(CAST(Quantity AS BIGINT)), Austria_sum(UnitPrice), United Arab Emirates_sum(CAST(Quantity AS BIGINT)), Lebanon_sum(CAST(Quantity AS BIGINT)), Japan_sum(CAST(Quantity AS BIGINT)), United Kingdom_sum(CAST(Quantity AS BIGINT)), Canada_sum(CAST(Quantity AS BIGINT)), Iceland_sum(CAST(CustomerID AS BIGINT)), Czech Republic_sum(CAST(Quantity AS BIGINT)), Lebanon_sum(CAST(CustomerID AS BIGINT)), Italy_sum(CAST(Quantity AS BIGINT)), Channel Islands_sum(CAST(CustomerID AS BIGINT)), USA_sum(UnitPrice), Singapore_sum(CAST(Quantity AS BIGINT)), EIRE_sum(CAST(CustomerID AS BIGINT)), European Community_sum(CAST(Quantity AS BIGINT)), Cyprus_sum(UnitPrice), Greece_sum(CAST(CustomerID AS BIGINT))];;\n'Project [date#3281, 'USA_sum(Quantity)]\n+- Filter (cast(date#3281 as string) > 2011-12-05)\n   +- Project [date#3281, __pivot_sum(CAST(`Quantity` AS BIGINT)) AS `sum(CAST(``Quantity`` AS BIGINT))`#3832[0] AS Australia_sum(CAST(Quantity AS BIGINT))#3989L, __pivot_sum(`UnitPrice`) AS `sum(``UnitPrice``)`#3910[0] AS Australia_sum(UnitPrice)#3990, __pivot_sum(CAST(`CustomerID` AS BIGINT)) AS `sum(CAST(``CustomerID`` AS BIGINT))`#3988[0] AS Australia_sum(CAST(CustomerID AS BIGINT))#3991L, __pivot_sum(CAST(`Quantity` AS BIGINT)) AS `sum(CAST(``Quantity`` AS BIGINT))`#3832[1] AS Austria_sum(CAST(Quantity AS BIGINT))#3992L, __pivot_sum(`UnitPrice`) AS `sum(``UnitPrice``)`#3910[1] AS Austria_sum(UnitPrice)#3993, __pivot_sum(CAST(`CustomerID` AS BIGINT)) AS `sum(CAST(``CustomerID`` AS BIGINT))`#3988[1] AS Austria_sum(CAST(CustomerID AS BIGINT))#3994L, __pivot_sum(CAST(`Quantity` AS BIGINT)) AS `sum(CAST(``Quantity`` AS BIGINT))`#3832[2] AS Bahrain_sum(CAST(Quantity AS BIGINT))#3995L, __pivot_sum(`UnitPrice`) AS `sum(``UnitPrice``)`#3910[2] AS Bahrain_sum(UnitPrice)#3996, __pivot_sum(CAST(`CustomerID` AS BIGINT)) AS `sum(CAST(``CustomerID`` AS BIGINT))`#3988[2] AS Bahrain_sum(CAST(CustomerID AS BIGINT))#3997L, __pivot_sum(CAST(`Quantity` AS BIGINT)) AS `sum(CAST(``Quantity`` AS BIGINT))`#3832[3] AS Belgium_sum(CAST(Quantity AS BIGINT))#3998L, __pivot_sum(`UnitPrice`) AS `sum(``UnitPrice``)`#3910[3] AS Belgium_sum(UnitPrice)#3999, __pivot_sum(CAST(`CustomerID` AS BIGINT)) AS `sum(CAST(``CustomerID`` AS BIGINT))`#3988[3] AS Belgium_sum(CAST(CustomerID AS BIGINT))#4000L, __pivot_sum(CAST(`Quantity` AS BIGINT)) AS `sum(CAST(``Quantity`` AS BIGINT))`#3832[4] AS Brazil_sum(CAST(Quantity AS BIGINT))#4001L, __pivot_sum(`UnitPrice`) AS `sum(``UnitPrice``)`#3910[4] AS Brazil_sum(UnitPrice)#4002, __pivot_sum(CAST(`CustomerID` AS BIGINT)) AS `sum(CAST(``CustomerID`` AS BIGINT))`#3988[4] AS Brazil_sum(CAST(CustomerID AS BIGINT))#4003L, __pivot_sum(CAST(`Quantity` AS BIGINT)) AS `sum(CAST(``Quantity`` AS BIGINT))`#3832[5] AS Canada_sum(CAST(Quantity AS BIGINT))#4004L, __pivot_sum(`UnitPrice`) AS `sum(``UnitPrice``)`#3910[5] AS Canada_sum(UnitPrice)#4005, __pivot_sum(CAST(`CustomerID` AS BIGINT)) AS `sum(CAST(``CustomerID`` AS BIGINT))`#3988[5] AS Canada_sum(CAST(CustomerID AS BIGINT))#4006L, __pivot_sum(CAST(`Quantity` AS BIGINT)) AS `sum(CAST(``Quantity`` AS BIGINT))`#3832[6] AS Channel Islands_sum(CAST(Quantity AS BIGINT))#4007L, __pivot_sum(`UnitPrice`) AS `sum(``UnitPrice``)`#3910[6] AS Channel Islands_sum(UnitPrice)#4008, __pivot_sum(CAST(`CustomerID` AS BIGINT)) AS `sum(CAST(``CustomerID`` AS BIGINT))`#3988[6] AS Channel Islands_sum(CAST(CustomerID AS BIGINT))#4009L, __pivot_sum(CAST(`Quantity` AS BIGINT)) AS `sum(CAST(``Quantity`` AS BIGINT))`#3832[7] AS Cyprus_sum(CAST(Quantity AS BIGINT))#4010L, __pivot_sum(`UnitPrice`) AS `sum(``UnitPrice``)`#3910[7] AS Cyprus_sum(UnitPrice)#4011, ... 91 more fields]\n      +- Aggregate [date#3281], [date#3281, pivotfirst(Country#1659, sum(CAST(`Quantity` AS BIGINT))#3752L, Australia, Austria, Bahrain, Belgium, Brazil, Canada, Channel Islands, Cyprus, Czech Republic, Denmark, EIRE, European Community, Finland, France, Germany, Greece, Hong Kong, Iceland, Israel, Italy, Japan, Lebanon, Lithuania, Malta, Netherlands, Norway, Poland, Portugal, RSA, Saudi Arabia, Singapore, Spain, Sweden, Switzerland, USA, United Arab Emirates, United Kingdom, Unspecified, 0, 0) AS __pivot_sum(CAST(`Quantity` AS BIGINT)) AS `sum(CAST(``Quantity`` AS BIGINT))`#3832, pivotfirst(Country#1659, sum(`UnitPrice`)#3753, Australia, Austria, Bahrain, Belgium, Brazil, Canada, Channel Islands, Cyprus, Czech Republic, Denmark, EIRE, European Community, Finland, France, Germany, Greece, Hong Kong, Iceland, Israel, Italy, Japan, Lebanon, Lithuania, Malta, Netherlands, Norway, Poland, Portugal, RSA, Saudi Arabia, Singapore, Spain, Sweden, Switzerland, USA, United Arab Emirates, United Kingdom, Unspecified, 0, 0) AS __pivot_sum(`UnitPrice`) AS `sum(``UnitPrice``)`#3910, pivotfirst(Country#1659, sum(CAST(`CustomerID` AS BIGINT))#3754L, Australia, Austria, Bahrain, Belgium, Brazil, Canada, Channel Islands, Cyprus, Czech Republic, Denmark, EIRE, European Community, Finland, France, Germany, Greece, Hong Kong, Iceland, Israel, Italy, Japan, Lebanon, Lithuania, Malta, Netherlands, Norway, Poland, Portugal, RSA, Saudi Arabia, Singapore, Spain, Sweden, Switzerland, USA, United Arab Emirates, United Kingdom, Unspecified, 0, 0) AS __pivot_sum(CAST(`CustomerID` AS BIGINT)) AS `sum(CAST(``CustomerID`` AS BIGINT))`#3988]\n         +- Aggregate [date#3281, Country#1659], [date#3281, Country#1659, sum(cast(Quantity#1655 as bigint)) AS sum(CAST(`Quantity` AS BIGINT))#3752L, sum(UnitPrice#1657) AS sum(`UnitPrice`)#3753, sum(cast(CustomerID#1658 as bigint)) AS sum(CAST(`CustomerID` AS BIGINT))#3754L]\n            +- Project [InvoiceNo#1652, StockCode#1653, Description#1654, Quantity#1655, InvoiceDate#1656, UnitPrice#1657, CustomerID#1658, Country#1659, to_date('InvoiceDate, Some(MM/d/yyyy H:mm)) AS date#3281]\n               +- Repartition 5, false\n                  +- Relation[InvoiceNo#1652,StockCode#1653,Description#1654,Quantity#1655,InvoiceDate#1656,UnitPrice#1657,CustomerID#1658,Country#1659] csv\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:110)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:107)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:107)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:79)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3407)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1335)\n\tat sun.reflect.GeneratedMethodAccessor42.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-3587757e50dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpivoted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"date > '2011-12-05'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"date\"\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\"`USA_sum(Quantity)`\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/share/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1318\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m         \"\"\"\n\u001b[0;32m-> 1320\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/share/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/share/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`USA_sum(Quantity)`' given input columns: [Cyprus_sum(CAST(Quantity AS BIGINT)), Australia_sum(UnitPrice), Canada_sum(UnitPrice), Lithuania_sum(CAST(Quantity AS BIGINT)), EIRE_sum(UnitPrice), Bahrain_sum(CAST(Quantity AS BIGINT)), Norway_sum(CAST(CustomerID AS BIGINT)), USA_sum(CAST(CustomerID AS BIGINT)), RSA_sum(CAST(Quantity AS BIGINT)), Canada_sum(CAST(CustomerID AS BIGINT)), United Kingdom_sum(CAST(CustomerID AS BIGINT)), Finland_sum(CAST(CustomerID AS BIGINT)), Israel_sum(CAST(CustomerID AS BIGINT)), Belgium_sum(CAST(Quantity AS BIGINT)), Lithuania_sum(CAST(CustomerID AS BIGINT)), Singapore_sum(CAST(CustomerID AS BIGINT)), Channel Islands_sum(UnitPrice), Belgium_sum(CAST(CustomerID AS BIGINT)), Malta_sum(CAST(Quantity AS BIGINT)), Netherlands_sum(CAST(CustomerID AS BIGINT)), Finland_sum(CAST(Quantity AS BIGINT)), Australia_sum(CAST(Quantity AS BIGINT)), Japan_sum(UnitPrice), Saudi Arabia_sum(UnitPrice), Switzerland_sum(CAST(CustomerID AS BIGINT)), United Arab Emirates_sum(CAST(CustomerID AS BIGINT)), Denmark_sum(CAST(CustomerID AS BIGINT)), Poland_sum(CAST(Quantity AS BIGINT)), Greece_sum(UnitPrice), Germany_sum(CAST(CustomerID AS BIGINT)), Netherlands_sum(UnitPrice), Portugal_sum(UnitPrice), Netherlands_sum(CAST(Quantity AS BIGINT)), Israel_sum(UnitPrice), Bahrain_sum(CAST(CustomerID AS BIGINT)), Iceland_sum(CAST(Quantity AS BIGINT)), Sweden_sum(UnitPrice), Israel_sum(CAST(Quantity AS BIGINT)), Switzerland_sum(UnitPrice), Norway_sum(UnitPrice), Hong Kong_sum(CAST(Quantity AS BIGINT)), Austria_sum(CAST(Quantity AS BIGINT)), Malta_sum(UnitPrice), United Kingdom_sum(UnitPrice), Australia_sum(CAST(CustomerID AS BIGINT)), Brazil_sum(UnitPrice), Belgium_sum(UnitPrice), France_sum(UnitPrice), Brazil_sum(CAST(CustomerID AS BIGINT)), Unspecified_sum(CAST(CustomerID AS BIGINT)), Unspecified_sum(UnitPrice), Hong Kong_sum(CAST(CustomerID AS BIGINT)), European Community_sum(UnitPrice), Spain_sum(CAST(Quantity AS BIGINT)), Japan_sum(CAST(CustomerID AS BIGINT)), Austria_sum(CAST(CustomerID AS BIGINT)), Czech Republic_sum(UnitPrice), Lithuania_sum(UnitPrice), Switzerland_sum(CAST(Quantity AS BIGINT)), Norway_sum(CAST(Quantity AS BIGINT)), Czech Republic_sum(CAST(CustomerID AS BIGINT)), Poland_sum(UnitPrice), Singapore_sum(UnitPrice), Italy_sum(CAST(CustomerID AS BIGINT)), Malta_sum(CAST(CustomerID AS BIGINT)), Spain_sum(UnitPrice), Sweden_sum(CAST(CustomerID AS BIGINT)), Poland_sum(CAST(CustomerID AS BIGINT)), Saudi Arabia_sum(CAST(Quantity AS BIGINT)), Denmark_sum(CAST(Quantity AS BIGINT)), USA_sum(CAST(Quantity AS BIGINT)), Germany_sum(CAST(Quantity AS BIGINT)), Spain_sum(CAST(CustomerID AS BIGINT)), date, Greece_sum(CAST(Quantity AS BIGINT)), United Arab Emirates_sum(UnitPrice), Lebanon_sum(UnitPrice), RSA_sum(CAST(CustomerID AS BIGINT)), Cyprus_sum(CAST(CustomerID AS BIGINT)), Saudi Arabia_sum(CAST(CustomerID AS BIGINT)), France_sum(CAST(Quantity AS BIGINT)), Hong Kong_sum(UnitPrice), RSA_sum(UnitPrice), Unspecified_sum(CAST(Quantity AS BIGINT)), Germany_sum(UnitPrice), Portugal_sum(CAST(CustomerID AS BIGINT)), European Community_sum(CAST(CustomerID AS BIGINT)), Iceland_sum(UnitPrice), Finland_sum(UnitPrice), France_sum(CAST(CustomerID AS BIGINT)), Brazil_sum(CAST(Quantity AS BIGINT)), Italy_sum(UnitPrice), Channel Islands_sum(CAST(Quantity AS BIGINT)), Denmark_sum(UnitPrice), Sweden_sum(CAST(Quantity AS BIGINT)), Portugal_sum(CAST(Quantity AS BIGINT)), Bahrain_sum(UnitPrice), EIRE_sum(CAST(Quantity AS BIGINT)), Austria_sum(UnitPrice), United Arab Emirates_sum(CAST(Quantity AS BIGINT)), Lebanon_sum(CAST(Quantity AS BIGINT)), Japan_sum(CAST(Quantity AS BIGINT)), United Kingdom_sum(CAST(Quantity AS BIGINT)), Canada_sum(CAST(Quantity AS BIGINT)), Iceland_sum(CAST(CustomerID AS BIGINT)), Czech Republic_sum(CAST(Quantity AS BIGINT)), Lebanon_sum(CAST(CustomerID AS BIGINT)), Italy_sum(CAST(Quantity AS BIGINT)), Channel Islands_sum(CAST(CustomerID AS BIGINT)), USA_sum(UnitPrice), Singapore_sum(CAST(Quantity AS BIGINT)), EIRE_sum(CAST(CustomerID AS BIGINT)), European Community_sum(CAST(Quantity AS BIGINT)), Cyprus_sum(UnitPrice), Greece_sum(CAST(CustomerID AS BIGINT))];;\\n'Project [date#3281, 'USA_sum(Quantity)]\\n+- Filter (cast(date#3281 as string) > 2011-12-05)\\n   +- Project [date#3281, __pivot_sum(CAST(`Quantity` AS BIGINT)) AS `sum(CAST(``Quantity`` AS BIGINT))`#3832[0] AS Australia_sum(CAST(Quantity AS BIGINT))#3989L, __pivot_sum(`UnitPrice`) AS `sum(``UnitPrice``)`#3910[0] AS Australia_sum(UnitPrice)#3990, __pivot_sum(CAST(`CustomerID` AS BIGINT)) AS `sum(CAST(``CustomerID`` AS BIGINT))`#3988[0] AS Australia_sum(CAST(CustomerID AS BIGINT))#3991L, __pivot_sum(CAST(`Quantity` AS BIGINT)) AS `sum(CAST(``Quantity`` AS BIGINT))`#3832[1] AS Austria_sum(CAST(Quantity AS BIGINT))#3992L, __pivot_sum(`UnitPrice`) AS `sum(``UnitPrice``)`#3910[1] AS Austria_sum(UnitPrice)#3993, __pivot_sum(CAST(`CustomerID` AS BIGINT)) AS `sum(CAST(``CustomerID`` AS BIGINT))`#3988[1] AS Austria_sum(CAST(CustomerID AS BIGINT))#3994L, __pivot_sum(CAST(`Quantity` AS BIGINT)) AS `sum(CAST(``Quantity`` AS BIGINT))`#3832[2] AS Bahrain_sum(CAST(Quantity AS BIGINT))#3995L, __pivot_sum(`UnitPrice`) AS `sum(``UnitPrice``)`#3910[2] AS Bahrain_sum(UnitPrice)#3996, __pivot_sum(CAST(`CustomerID` AS BIGINT)) AS `sum(CAST(``CustomerID`` AS BIGINT))`#3988[2] AS Bahrain_sum(CAST(CustomerID AS BIGINT))#3997L, __pivot_sum(CAST(`Quantity` AS BIGINT)) AS `sum(CAST(``Quantity`` AS BIGINT))`#3832[3] AS Belgium_sum(CAST(Quantity AS BIGINT))#3998L, __pivot_sum(`UnitPrice`) AS `sum(``UnitPrice``)`#3910[3] AS Belgium_sum(UnitPrice)#3999, __pivot_sum(CAST(`CustomerID` AS BIGINT)) AS `sum(CAST(``CustomerID`` AS BIGINT))`#3988[3] AS Belgium_sum(CAST(CustomerID AS BIGINT))#4000L, __pivot_sum(CAST(`Quantity` AS BIGINT)) AS `sum(CAST(``Quantity`` AS BIGINT))`#3832[4] AS Brazil_sum(CAST(Quantity AS BIGINT))#4001L, __pivot_sum(`UnitPrice`) AS `sum(``UnitPrice``)`#3910[4] AS Brazil_sum(UnitPrice)#4002, __pivot_sum(CAST(`CustomerID` AS BIGINT)) AS `sum(CAST(``CustomerID`` AS BIGINT))`#3988[4] AS Brazil_sum(CAST(CustomerID AS BIGINT))#4003L, __pivot_sum(CAST(`Quantity` AS BIGINT)) AS `sum(CAST(``Quantity`` AS BIGINT))`#3832[5] AS Canada_sum(CAST(Quantity AS BIGINT))#4004L, __pivot_sum(`UnitPrice`) AS `sum(``UnitPrice``)`#3910[5] AS Canada_sum(UnitPrice)#4005, __pivot_sum(CAST(`CustomerID` AS BIGINT)) AS `sum(CAST(``CustomerID`` AS BIGINT))`#3988[5] AS Canada_sum(CAST(CustomerID AS BIGINT))#4006L, __pivot_sum(CAST(`Quantity` AS BIGINT)) AS `sum(CAST(``Quantity`` AS BIGINT))`#3832[6] AS Channel Islands_sum(CAST(Quantity AS BIGINT))#4007L, __pivot_sum(`UnitPrice`) AS `sum(``UnitPrice``)`#3910[6] AS Channel Islands_sum(UnitPrice)#4008, __pivot_sum(CAST(`CustomerID` AS BIGINT)) AS `sum(CAST(``CustomerID`` AS BIGINT))`#3988[6] AS Channel Islands_sum(CAST(CustomerID AS BIGINT))#4009L, __pivot_sum(CAST(`Quantity` AS BIGINT)) AS `sum(CAST(``Quantity`` AS BIGINT))`#3832[7] AS Cyprus_sum(CAST(Quantity AS BIGINT))#4010L, __pivot_sum(`UnitPrice`) AS `sum(``UnitPrice``)`#3910[7] AS Cyprus_sum(UnitPrice)#4011, ... 91 more fields]\\n      +- Aggregate [date#3281], [date#3281, pivotfirst(Country#1659, sum(CAST(`Quantity` AS BIGINT))#3752L, Australia, Austria, Bahrain, Belgium, Brazil, Canada, Channel Islands, Cyprus, Czech Republic, Denmark, EIRE, European Community, Finland, France, Germany, Greece, Hong Kong, Iceland, Israel, Italy, Japan, Lebanon, Lithuania, Malta, Netherlands, Norway, Poland, Portugal, RSA, Saudi Arabia, Singapore, Spain, Sweden, Switzerland, USA, United Arab Emirates, United Kingdom, Unspecified, 0, 0) AS __pivot_sum(CAST(`Quantity` AS BIGINT)) AS `sum(CAST(``Quantity`` AS BIGINT))`#3832, pivotfirst(Country#1659, sum(`UnitPrice`)#3753, Australia, Austria, Bahrain, Belgium, Brazil, Canada, Channel Islands, Cyprus, Czech Republic, Denmark, EIRE, European Community, Finland, France, Germany, Greece, Hong Kong, Iceland, Israel, Italy, Japan, Lebanon, Lithuania, Malta, Netherlands, Norway, Poland, Portugal, RSA, Saudi Arabia, Singapore, Spain, Sweden, Switzerland, USA, United Arab Emirates, United Kingdom, Unspecified, 0, 0) AS __pivot_sum(`UnitPrice`) AS `sum(``UnitPrice``)`#3910, pivotfirst(Country#1659, sum(CAST(`CustomerID` AS BIGINT))#3754L, Australia, Austria, Bahrain, Belgium, Brazil, Canada, Channel Islands, Cyprus, Czech Republic, Denmark, EIRE, European Community, Finland, France, Germany, Greece, Hong Kong, Iceland, Israel, Italy, Japan, Lebanon, Lithuania, Malta, Netherlands, Norway, Poland, Portugal, RSA, Saudi Arabia, Singapore, Spain, Sweden, Switzerland, USA, United Arab Emirates, United Kingdom, Unspecified, 0, 0) AS __pivot_sum(CAST(`CustomerID` AS BIGINT)) AS `sum(CAST(``CustomerID`` AS BIGINT))`#3988]\\n         +- Aggregate [date#3281, Country#1659], [date#3281, Country#1659, sum(cast(Quantity#1655 as bigint)) AS sum(CAST(`Quantity` AS BIGINT))#3752L, sum(UnitPrice#1657) AS sum(`UnitPrice`)#3753, sum(cast(CustomerID#1658 as bigint)) AS sum(CAST(`CustomerID` AS BIGINT))#3754L]\\n            +- Project [InvoiceNo#1652, StockCode#1653, Description#1654, Quantity#1655, InvoiceDate#1656, UnitPrice#1657, CustomerID#1658, Country#1659, to_date('InvoiceDate, Some(MM/d/yyyy H:mm)) AS date#3281]\\n               +- Repartition 5, false\\n                  +- Relation[InvoiceNo#1652,StockCode#1653,Description#1654,Quantity#1655,InvoiceDate#1656,UnitPrice#1657,CustomerID#1658,Country#1659] csv\\n\""
     ]
    }
   ],
   "source": [
    "# pivoted.where(\"date > '2011-12-05'\").select(\"date\" ,\"`USA_sum(Quantity)`\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
